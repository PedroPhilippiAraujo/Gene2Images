---
title: "Gene2Images"
author: "Pedro Philippi Araujo, Lea Feilberg, Lennart Koch, Xenia Kukushkina"
format: 
  html: 
    embed: true
execute:
  environment: metabric   
kernel: metabric
---

## Introduction

This code will be used to acquire data and visualize it.

## Data Acquisition and Extraction
```{python}
import os
import urllib.request
import tarfile

url = "https://cbioportal-datahub.s3.amazonaws.com/brca_metabric.tar.gz"
archive_path = "../data/brca_metabric.tar.gz"

if not (os.path.exists("../data")):
    os.mkdir("../data")
    print("Data directory created")

if not (os.path.exists("../artifacts")):
    os.mkdir("../artifacts")
    print("Artifact directory created")

if (os.path.exists(archive_path)):
    print("Data already downloaded")
else:
    print("Downloading Data")
    urllib.request.urlretrieve(url, archive_path)
    print("Download Complete")

if not (os.path.exists("../data/brca_metabric")):
    with tarfile.open(archive_path, "r") as tar:
        print("Extracting archive")
        tar.extractall(path="../data/")
        print("Extraction complete")
else:
    print("Data already extracted")

```

## Imports for Data Framing and Plotting 

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
```

## Data Framing

```{python}
metabric_dir = "../data/brca_metabric"
data_path = f"{metabric_dir}/data_mrna_illumina_microarray_zscores_ref_diploid_samples.txt"
metadata_path = f"{metabric_dir}/data_clinical_patient.txt"

#Read gene expression data
expr_df = pd.read_csv(data_path, sep="\t")
#safe one version with Hugo_symbols
expr_with_Hugo_df = expr_df

expr_df = expr_df.set_index("Entrez_Gene_Id")

expr_df = expr_df.drop(columns=["Hugo_Symbol"])

expr_transposed_df = expr_df.T

meta_df = pd.read_csv(metadata_path, sep="\t", comment="#")
meta_df = meta_df.set_index("PATIENT_ID")

```

```{python}
print(f"Expression data shape: {expr_transposed_df.shape}")
print(f"Metadata shape: {meta_df.shape}")
```

## Creating one Dataframe with only PAM50 Genes

```{python}
# only runs first time without Error, ignore Error if you run it a second time, you can still use PAM50_transposed_df in next python fields
expr_with_Hugo_df.set_index('Hugo_Symbol', inplace=True)
selected_symbols = [symbol for symbol in ["ACTR3B", "ANLN", "BAG1", "BCL2", "BIRC5", "BLVRA", "CCNB1","CCNE1", "CDC20", "CDC6", "CDH3", "CENPF", "CEP55", "CXXC5", "EGFR", "ERBB2", "ESR1", "EXO1", "FGFR4", "FOXA1", "FOXC1", "GPR160", "GRB7", "KIF2C", "KRT14", "KRT17", "KRT5", "MAPT", "MDM2", "MELK", "MIA", "MKI67", "MLPH", "MMP11", "MYBL2", "MYC", "NAT1", "NDC80", "NUF2", "ORC6L", "PGR", "PHGDH", "PTTG1", "RRM2", "SFRP1", "SLC39A6", "TMEM45B", "TYMS", "UBE2C", "UBE2T"] if symbol in expr_with_Hugo_df.index]
PAM50_df = expr_with_Hugo_df.loc[selected_symbols]
PAM50_df = PAM50_df.set_index("Entrez_Gene_Id")
PAM50_transposed_df = PAM50_df.T

print("Dataframe created")

```

## Metadata Plots with Input Data before Cleaning

```{python}
#| code-fold: true
# Ensure output subfolder exists for images saved by this section
import os
os.makedirs("../artifacts/images", exist_ok=True)

#Age at Point of Diagnosis distribution 

#plt.figure()
#sns.histplot(meta_df["AGE_AT_DIAGNOSIS"], bins=100)
#plt.title("Histogram of Age at Diagnosis")
#plt.xlabel("Age")
#plt.ylabel("Count")
#plt.tight_layout()
#plt.savefig("../artifacts/images/AgePlot.png")
#plt.show()
#plt.close()
#
## #Chemotherapy
#
#plt.figure()
#sns.histplot(meta_df["CHEMOTHERAPY"], bins=2)
#plt.title("Histogram of Chemotherapy")
#plt.xlabel("Patient had Chemotherapy")
#plt.ylabel("Number of Patients")
#plt.tight_layout()
#plt.savefig("../artifacts/images/Chemotherapy.png")
#plt.show()
#plt.close()

# #Distribution of Claudin-Subtypes  

plt.figure()
sns.histplot(meta_df["CLAUDIN_SUBTYPE"], bins=7)
plt.title("Distribution of Claudin Subtypes")
plt.xlabel("Claudin Subtype")
plt.ylabel("Number of Patients")
plt.tight_layout()
plt.savefig("../artifacts/images/Claudine_Distribution.png")
plt.show()
plt.close()

```

## Drop bad values in Expression Data

```{python}
print(f'data shape before dropna: {expr_transposed_df.shape}')
expr_transposed_df = expr_transposed_df.dropna()
print(f"data shape after: {expr_transposed_df.shape}")
```


## Data Summary Statistics

```{python}
#| code-fold: true
#| output: false

print(expr_transposed_df.describe())
print(expr_transposed_df.head())
print(meta_df.describe())
print(meta_df['CLAUDIN_SUBTYPE'].value_counts())
```

## Data Plotting - Claudin Subtypes vs Gene Expression

```{python}
#| code-fold: true

# Prepare data with claudin subtypes for visualization
expr_with_claudin = expr_transposed_df.copy()
expr_with_claudin['CLAUDIN_SUBTYPE'] = meta_df.loc[expr_with_claudin.index, 'CLAUDIN_SUBTYPE']

# Remove rows with missing claudin subtype
expr_with_claudin = expr_with_claudin.dropna(subset=['CLAUDIN_SUBTYPE'])

# Select top variable genes for visualization
gene_variances = expr_with_claudin.iloc[:, :-1].var()
top_genes = gene_variances.nlargest(6).index.tolist()

# Create violin plots for top genes across claudin subtypes
fig, axes = plt.subplots(2, 3, figsize=(16, 10))
axes = axes.flatten()

for idx, gene in enumerate(top_genes):
    sns.violinplot(data=expr_with_claudin, x='CLAUDIN_SUBTYPE', y=gene, ax=axes[idx], hue='CLAUDIN_SUBTYPE', palette='Set2', legend=False)
    axes[idx].set_title(f'Gene {gene} Expression by Claudin Subtype', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Claudin Subtype')
    axes[idx].set_ylabel('Expression (z-score)')
    axes[idx].tick_params(axis='x', rotation=45)

plt.suptitle('Top Variable Genes Expression Across Claudin Subtypes', fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig("../artifacts/images/Claudin_Gene_Violin_Plots.png", dpi=300, bbox_inches='tight')
plt.show()
plt.close()
```


## Drop columns in Meta - Concentration on ID and CLAUDIN

```{python}
print(f'metadata shape before dropna: {meta_df.shape}')
print(f'Claudin Subtypes before dropna: {meta_df["CLAUDIN_SUBTYPE"].value_counts()}')

meta_reduced_df = meta_df.drop(columns=['LYMPH_NODES_EXAMINED_POSITIVE',"NPI","CELLULARITY",
"CHEMOTHERAPY","COHORT","ER_IHC","HER2_SNP6","HORMONE_THERAPY","INFERRED_MENOPAUSAL_STATE"	
,"SEX","INTCLUST","AGE_AT_DIAGNOSIS","OS_MONTHS","OS_STATUS","THREEGENE","VITAL_STATUS","LATERALITY","RADIO_THERAPY","HISTOLOGICAL_SUBTYPE","BREAST_SURGERY","RFS_MONTHS","RFS_STATUS"])
# Dropnas of Meta here because we only need data of Claudine Typ, so we don't care if NA is in some other column
meta_reduced_df = meta_reduced_df.dropna()
print(f"metadata shape after: {meta_reduced_df.shape}")
print(meta_reduced_df.shape)
print(expr_transposed_df.shape)


```

## Merge Data with reduced Metadata on Patient-ID

| Claudin Subtype | Integer Code |
|---|---|
| LumA | 6 |
| LumB | 5 |
| Her2 | 4 |
| claudin-low | 3 |
| Basal | 2 |
| Normal | 1 |
| NC | 0 |

*Note: LumA gets the highest number (6) as it's most frequent in the data*

```{python}
#set Index Name of transposed expr_data the same as meta data
expr_transposed_df.index.name = 'PATIENT_ID'
#merge both sets together 
merged_df=expr_transposed_df.merge(meta_reduced_df, how='left', on="PATIENT_ID")
#change Subtypes from String to numbers
merged_df["CLAUDIN_SUBTYPE"] = merged_df["CLAUDIN_SUBTYPE"].replace({"LumA": "6", "LumB": "5","Her2": "4","claudin-low": "3","Basal": "2","Normal": "1","NC": "0"})# inplace = True)
print(merged_df.shape)
print("Data Merged")
```


## Multinomial Regression

```{python}

#Tensor Conversion

X = merged_df.iloc[:-400,0:-1].values #X = all gene expression data

X_test = merged_df.iloc[-400:, 0:-1].values 

Y = merged_df["CLAUDIN_SUBTYPE"].iloc[:-400].values #Y = Claudin subtype
Y = Y.astype(int)

Y_test = merged_df["CLAUDIN_SUBTYPE"].iloc[-400:].values
Y_test = Y_test.astype(int)


X = torch.tensor(X, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.long)
Y_test = torch.tensor(Y_test, dtype=torch.long)


print("Conversion to tensor done")
```

```{python}

#model training

n_epochs = 500
model = nn.Linear(20603, 7)  # change: n_in = 20603 (features), n_out = 7 (classes)
loss_func = nn.CrossEntropyLoss()  # Cross-Entropy

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) 

model.train()
for epoch in range(n_epochs):
    # model predictions are interpreted as z = logits
    z = model(X)  
    loss = loss_func(z, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("Fitted parameters:")
print("Intercept (bias):", model.bias.detach().numpy())

## Notice more weights
print("Coefficients (weights):", model.weight.detach().numpy())
print('Loss: ', loss)    
```


```{python}
#Model Evaluation
model.eval()
with torch.no_grad():
    z = model(X_test)
    y_prob = z.softmax(dim=1) # for easier interpretation
    y_pred = z.argmax(dim=1)  # class predition = largest logit
    acc = (y_pred == Y_test).float().mean().item()

print('y_true:     ', Y[:7])
print('z (logits): ', z[:7])
print('y_prob:     ', y_prob[:7])
print('y_pred:     ', y_pred[:7])

print(f"\n\nTraining accuracy: {acc:.3f}")

```

```{python}
#Confusion Matrix

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Map numeric labels to actual class names
class_names = {0: "NC", 1: "Normal", 2: "Basal", 3: "claudin-low", 4: "Her2", 5: "LumB", 6: "LumA"}
labels = [0, 1, 2, 3, 4, 5, 6]
label_names = [class_names[i] for i in labels]

cm = confusion_matrix(Y_test, y_pred.numpy())

plt.figure(figsize=(10, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
disp.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix - Multinomial Logistic Regression', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
plt.close()

# Print detailed classification report
print("\nClassification Report:")
print(classification_report(Y_test.numpy(), y_pred.numpy(), 
                          target_names=label_names, 
                          labels=labels,
                          zero_division=0))
```

## Deep Learning

```{python}
#Tensor Conversion

X = merged_df.iloc[:-400,0:-1].values #X = all gene expression data

X_test = merged_df.iloc[-400:, 0:-1].values 

Y = merged_df["CLAUDIN_SUBTYPE"].iloc[:-400].values #Y = Claudin subtype
Y = Y.astype(int)

Y_test = merged_df["CLAUDIN_SUBTYPE"].iloc[-400:].values
Y_test = Y_test.astype(int)


X = torch.tensor(X, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.long)
Y_test = torch.tensor(Y_test, dtype=torch.long)


print("Conversion to tensor done")
```

```{python}
#Model Definition

class Classifier(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=512 ):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(1024,512),
            nn.BatchNorm1d(512),
            nn.ReLU(),

        )

        self.latent = nn.Linear(512, hidden_dim)
        self.latent_bn = nn.BatchNorm1d(hidden_dim)
        self.classifier = nn.Linear(hidden_dim, output_dim)

#    def forward(self, x):
#        return self.net(x)

    def forward(self, x):
        # return NN(x) _and_ hidden_ state z_bn
        x = self.encoder(x)
        z = self.latent(x)
        z_bn = self.latent_bn(z)  # latent representation (approx. normal)
        out = self.classifier(z_bn)
        return out, z_bn  # return both for access later


#model = Classifier(input_dim, hidden_dim, output_dim)
```

```{python}
#model training

n_epochs = 30
losslist = []
model = Classifier(input_dim = 20603, output_dim=7)  # change: n_in = 20603 (features), n_out = 7 (classes)
loss_func = nn.CrossEntropyLoss()  # Cross-Entropy

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) 

model.train()
for epoch in range(n_epochs):
    # model predictions are interpreted as z = logits
    z_out, z_bn = model(X)  
    loss = loss_func(z_out, Y)
    losslist.append(loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   
```

```{python}
#Model Evaluation

model.eval()
with torch.no_grad():
    z_out, z_bn = model(X_test)
    y_prob = z_out.softmax(dim=1) # for easier interpretation
    y_pred = z_out.argmax(dim=1)  # class predition = largest logit
    acc = (y_pred == Y_test).float().mean().item()

print('y_true:     ', Y[:7])
#print('z (logits): ', z_out[:7])
print('y_prob:     ', y_prob[:7])
print('y_pred:     ', y_pred[:7])

print(f"\n\nTraining accuracy: {acc:.3f}")
```

## Model Evaluation - Visualizations

```{python}
#| code-fold: true

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Loss plot
xs = [x for x in range(len(losslist))]
plt.figure(figsize=(8, 5))
plt.plot(xs, losslist, linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Training Loss (Cross-Entropy)', fontsize=12)
plt.title('Deep Learning Model - Training Loss Over Time', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
plt.close()

# Confusion Matrix
class_names = {0: "NC", 1: "Normal", 2: "Basal", 3: "claudin-low", 4: "Her2", 5: "LumB", 6: "LumA"}
labels = sorted(torch.unique(Y_test).tolist())
label_names = [class_names[i] for i in labels]

cm = confusion_matrix(Y_test, y_pred.numpy())

plt.figure(figsize=(10, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
disp.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix - Deep Learning Model', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
plt.close()

# Print detailed classification report
print("\nClassification Report:")
print(classification_report(Y_test.numpy(), y_pred.numpy(), 
                          target_names=label_names, 
                          labels=labels,
                          zero_division=0))
```

# Save Z_BN

```{python}
model.eval()
with torch.no_grad():
    # right now its only saving the test zbn (data of 400 patients)
    _, z_bn_final = model(X_test)          
    torch.save(z_bn_final.detach().cpu(), 'z_bn_final.pt')
    print(' Finales z_bn gespeichert → z_bn_final.pt')
```

# Visualize Z Distribution

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Convert to numpy for visualization
z_bn_np = z_bn_final.detach().cpu().numpy()

print(f"Z_BN Shape: {z_bn_np.shape}")
print(f"Z_BN Statistics:")
print(f"  Mean: {z_bn_np.mean():.4f}")
print(f"  Std: {z_bn_np.std():.4f}")
print(f"  Min: {z_bn_np.min():.4f}")
print(f"  Max: {z_bn_np.max():.4f}")
```

## 1. Distribution of Z values (Histogram)

```{python}
#| code-fold: true
# Plot histogram of all Z values
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(z_bn_np.flatten(), bins=100, alpha=0.7, color='steelblue', edgecolor='black')
plt.axvline(z_bn_np.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {z_bn_np.mean():.2f}')
plt.xlabel('Z values')
plt.ylabel('Frequency')
plt.title('Distribution of all Z_BN values')
plt.legend()
plt.grid(True, alpha=0.3)

# Q-Q plot to check normality
plt.subplot(1, 2, 2)
from scipy import stats
sample_values = z_bn_np.flatten()
stats.probplot(sample_values, dist="norm", plot=plt)
plt.title('Q-Q Plot (Check for Normal Distribution)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 2. Distribution per Dimension

```{python}
#| code-fold: true
# Show distribution statistics for first 10 dimensions
fig, axes = plt.subplots(2, 5, figsize=(18, 8))
axes = axes.flatten()

for i in range(min(10, z_bn_np.shape[1])):
    axes[i].hist(z_bn_np[:, i], bins=30, alpha=0.7, color='steelblue', edgecolor='black')
    axes[i].axvline(z_bn_np[:, i].mean(), color='red', linestyle='--', linewidth=1.5)
    axes[i].set_title(f'Dimension {i+1}\nμ={z_bn_np[:, i].mean():.2f}, σ={z_bn_np[:, i].std():.2f}')
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Count')
    axes[i].grid(True, alpha=0.3)

plt.suptitle('Distribution of First 10 Z_BN Dimensions', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()
```


# Model Save
```{python}
#Saving the model at artifacts
path = '../artifacts/model.pt'

torch.save(model, path)
print(f"Model saved at {path}")

```

# Divide into Full dim and PCA representation


# Switching between settings of meta data
```{python}

from sklearn.model_selection import train_test_split


#Takes a metadata column name (like "THREEGENE" or "CLAUDIN_SUBTYPE").

#Cleans it (drops NA, removes invalid categories like "NC" or 0).

#Merges it with expression data.

#Splits it automatically into training and test sets (80/20).

def show_metadata_column(meta_df, column_name):
    if column_name in meta_df.columns:

        #remove empty values
        meta_clean = meta_df[[column_name]].dropna()

        #remove invalid values 'NC' and '0'
        meta_clean = meta_clean[~meta_clean[column_name].isin(["NC", "0"])]

        #Merge with expression data
        merged_df = expr_transposed_df.merge(meta_clean, left_index=True, right_index=True)

        #Training and test sets (80/20)
        X = merged_df.iloc[:, :-1]
        Y = merged_df.iloc[:, -1]

        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

        #sample count
        n_total = len(X)

        #print
        print(f"Metadata column '{column_name}' cleaned and merged with expression data")
        print(f"Total samples: {n_total}")
        print(f"Training set: {X_train.shape[0]} samples")
        print(f"Test set: {X_test.shape[0]} samples")

        return X_train, X_test, Y_train, Y_test

    else:
        print(f"Column '{column_name}' does not exist in meta_df")
```

# PCA representation
```{python}
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

#z_bn (batch normalization)
z_bn_np = z_bn.detach().numpy()

#Y_test = tensor with labels
labels = Y_test.numpy()

#reduce PCA data to 2D
pca = PCA(n_components=2)
z_2d = pca.fit_transform(z_bn_np)

plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=z_2d[:, 0],
    y=z_2d[:, 1],
    hue=labels,
    palette='tab10',
    alpha=0.8

)
plt.title("2D PCA of 512D BatchNorm Latents (Cancer Types)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.axhline(0, color='grey', linewidth=0.5)
plt.axvline(0, color='grey', linewidth=0.5)
plt.tight_layout()
plt.show()

```

# long-form DataFrame 
pca_long_df = pca_df.melt(
    id_vars="Claudin_Subtype",
    value_vars=["PC1", "PC2"],
    var_name="PC_Component",
    value_name="Value"
)

plt.figure(figsize=(14, 7))
sns.set(style="whitegrid", palette="Set2")

sns.violinplot(
    data=pca_long_df,
    x="Claudin_Subtype",
    y="Value",
    hue="PC_Component",
    dodge=True
    inner="quartile",   
    palette={"PC1":"#1f77b4", "PC2":"#ff7f0e"}
)

plt.xlabel("Claudin Subtype", fontsize=12, fontweight="bold")
plt.ylabel("PCA Value", fontsize=12, fontweight="bold")
plt.title("Distribution of PCA Components (PC1 & PC2) by Claudin Subtype before dropping NA values", fontsize=14, fontweight="bold")
plt.xticks(rotation=45)
plt.legend(title="PCA Component", loc="upper right")
plt.tight_layout()
plt.show()
```