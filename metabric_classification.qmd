---
title: Gene2Images
format: html
---

## Introduction

This code will be use to acquire data and visualize it.

## Data Acquisition and Extraction

```{python}

import os
import urllib.request
import tarfile

url = "https://cbioportal-datahub.s3.amazonaws.com/brca_metabric.tar.gz"
archive_path = "../data/brca_metabric.tar.gz"

if not (os.path.exists("../data")):
    os.mkdir("../data")
    print("Data directory created")

if not (os.path.exists("../artifacts")):
    os.mkdir("../artifacts")
    print("Artifact directory created")

if (os.path.exists(archive_path)):
    print("Data already downloaded")
else:
    print("Downloading Data")
    urllib.request.urlretrieve(url, archive_path)
    print("Download Complete")

if not (os.path.exists("../data/brca_metabric")):
    with tarfile.open(archive_path, "r") as tar:
        print("Extracting archive")
        tar.extractall(path="../data/")
        print("Extraction complete")
else:
    print("Data already extracted")

```

## Imports for Data Framing and Plotting 

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
```

## Data Framing

```{python}
metabric_dir = "../data/brca_metabric"
data_path = f"{metabric_dir}/data_mrna_illumina_microarray_zscores_ref_diploid_samples.txt"
metadata_path = f"{metabric_dir}/data_clinical_patient.txt"

#Read gene expression data
expr_df = pd.read_csv(data_path, sep="\t")
#safe one version with Hugo_symbols
expr_with_Hugo_df = expr_df

expr_df = expr_df.set_index("Entrez_Gene_Id")

expr_df = expr_df.drop(columns=["Hugo_Symbol"])

expr_transposed_df = expr_df.T

meta_df = pd.read_csv(metadata_path, sep="\t", comment="#")
meta_df = meta_df.set_index("PATIENT_ID")

```

## Creating one Dataframe with only PAM50 Genes

```{python}
# only runs first time without Error, ignore Error if you run it a second time, you can still use PAM50_transposed_df in next python fields
expr_with_Hugo_df.set_index('Hugo_Symbol', inplace=True)
selected_symbols = [symbol for symbol in ["ACTR3B", "ANLN", "BAG1", "BCL2", "BIRC5", "BLVRA", "CCNB1","CCNE1", "CDC20", "CDC6", "CDH3", "CENPF", "CEP55", "CXXC5", "EGFR", "ERBB2", "ESR1", "EXO1", "FGFR4", "FOXA1", "FOXC1", "GPR160", "GRB7", "KIF2C", "KRT14", "KRT17", "KRT5", "MAPT", "MDM2", "MELK", "MIA", "MKI67", "MLPH", "MMP11", "MYBL2", "MYC", "NAT1", "NDC80", "NUF2", "ORC6L", "PGR", "PHGDH", "PTTG1", "RRM2", "SFRP1", "SLC39A6", "TMEM45B", "TYMS", "UBE2C", "UBE2T"] if symbol in expr_with_Hugo_df.index]
PAM50_df = expr_with_Hugo_df.loc[selected_symbols]
PAM50_df = PAM50_df.set_index("Entrez_Gene_Id")
PAM50_transposed_df = PAM50_df.T

print("Dataframe created")

```

## Metadata Plots with Input Data before Cleaning

```{python}
#| eval: false

#Age at Point of Diagnosis distribution 

# plt.figure()
# sns.histplot(meta_df["AGE_AT_DIAGNOSIS"], bins=100)
# plt.title("Histogram of Age at Diagnosis")
# plt.xlabel("Age")
# plt.ylabel("Count")
# plt.tight_layout()
# plt.savefig("../artifacts/images/AgePlot.png")
# plt.show()
# plt.close()

# #Chemotherapy

# plt.figure()
# sns.histplot(meta_df["CHEMOTHERAPY"], bins=2)
# plt.title("Histogram of Chemotherapy")
# plt.xlabel("Patient had Chemotherapy")
# plt.ylabel("Number of Patients")
# plt.tight_layout()
# plt.savefig("../artifacts/images/Chemotherapy.png")
# plt.show()
# plt.close()

# #Distribution of Claudin-Subtypes  

# plt.figure()
# sns.histplot(meta_df["CLAUDIN_SUBTYPE"], bins=7)
# plt.title("Distribution of Claudin Subtypes")
# plt.xlabel("Claudin Subtype")
# plt.ylabel("Number of Patients")
# plt.tight_layout()
# plt.savefig("../artifacts/images/Claudine_Distribution.png")
# plt.show()
# plt.close()

# #THREEGENE
# #Distribution of Threegene

# plt.figure()
# sns.histplot(meta_df["THREEGENE"], bins=4)
# plt.title("Distribution of Threegene")
# plt.xlabel("Threegene")
# plt.ylabel("Number of Patients")
# plt.tight_layout()
# plt.xticks(fontsize=7)  
# plt.savefig("../artifacts/images/ThreeGene_Distribution.png")
# plt.show()
# plt.close()

```

## Expression Data Plots before cleaning the Dataset

```{python}

#| eval: false


#Shows Distribution of values of the first 40 Genes to check if Dataset is normalized

# columns_to_plot = expr_transposed_df.columns[:40]
# plt.figure(figsize=(15, 30)) 
# for i, column in enumerate(columns_to_plot):
#     plt.subplot(10, 4, i + 1)  
#     sns.histplot(expr_transposed_df[column].dropna(), bins=100)  
#     plt.title(f'Entrez Gene ID: {column}')  
#     plt.xlabel("Value")
#     plt.ylabel("Count")
#     plt.xlim(-10, 10) 
#     plt.ylim(0,200) 
# plt.tight_layout()  
# plt.savefig("../artifacts/images/Distribution_of_40_Genes.png", dpi=300)  
# plt.show()  
# plt.close()

```

## Drop bad values in Expression Data

```{python}
print(f'data shape before dropna: {expr_transposed_df.shape}')
#print(f'metadata shape before dropna: {meta_df.shape}')
#print(f'Claudin Subtypes before dropna: {meta_df["CLAUDIN_SUBTYPE"].value_counts()}')
expr_transposed_df = expr_transposed_df.dropna()
#meta_df = meta_df.dropna()
print(f"data shape after: {expr_transposed_df.shape}")
#print(f"metadata shape after: {meta_df.shape}")
#print(f'Claudin Subtypes after: {meta_df["CLAUDIN_SUBTYPE"].value_counts()}')

print("Bad Values dropped")

```


## Data Plotting

```{python}
#print(expr_transposed_df.describe())
#print(expr_transposed_df.head())
#print(meta_df.describe())

#print(meta_df['CLAUDIN_SUBTYPE'].value_counts())


expr_transposed_df_long = expr_transposed_df.melt(var_name="Feature", value_name="Value")

#Gene Plot
# plt.figure()
# sns.histplot(expr_transposed_df_long["Value"], bins=50)
# plt.title("Histogram")
# plt.xlabel("Value")
# plt.ylabel("Count")
# plt.tight_layout()
# plt.show()
# plt.savefig("../artifacts/images/GenePlot.png")
# plt.close()

#Claudin Subtype Plot
#plt.figure()
#sns.histplot(meta_df["CLAUDIN_SUBTYPE"], bins=7)    
#plt.title("Claudin Subtype Count")
#plt.xlabel("Subtype")
#plt.ylabel("Count")
#plt.tight_layout()
#plt.show()
#plt.savefig("../artifacts/images/Claudin.png")
#plt.close()


plt.figure()
sns.heatmap(expr_transposed_df.iloc[:10, :15], cmap="Blues", cbar_kws={'label': 'Value'})
plt.tight_layout()
plt.show()
plt.close()
```

## Plot PAM50 Genes

```{python}
#| eval: false
#PAM50_transposed_df

#Shows Distribution of values of PAM50_Genes

# columns_to_plot = PAM50_transposed_df.columns[:50]
# plt.figure(figsize=(15, 30)) 
# for i, column in enumerate(columns_to_plot):
#     plt.subplot(13, 4, i + 1)  
#     sns.histplot(PAM50_transposed_df[column].dropna(), bins=100)  
#     plt.title("Distribution of PAM50 Values")
#     plt.title(f'Entrez Gene ID: {column}')  
#     plt.xlabel("Value")
#     plt.ylabel("Count")
#     plt.xlim(-10, 10) 
#     plt.ylim(0,200) 
# plt.tight_layout()  
# plt.savefig("../artifacts/images/Distribution_of_PAM50_Genes.png", dpi=300)  
# plt.show()  
# plt.close()

# # Plot test Heatmap to compare to test Heatmap of Expression Set

# plt.figure()
# sns.heatmap(PAM50_transposed_df.iloc[:10, :15], cmap="Blues", cbar_kws={'label': 'Value'})
# plt.tight_layout()
# plt.show()

```


## Drop columns in Meta - Concentration on ID and CLAUDIN

```{python}
print(f'metadata shape before dropna: {meta_df.shape}')
print(f'Claudin Subtypes before dropna: {meta_df["CLAUDIN_SUBTYPE"].value_counts()}')

meta_reduced_df = meta_df.drop(columns=['LYMPH_NODES_EXAMINED_POSITIVE',"NPI","CELLULARITY",
"CHEMOTHERAPY","COHORT","ER_IHC","HER2_SNP6","HORMONE_THERAPY","INFERRED_MENOPAUSAL_STATE"	
,"SEX","INTCLUST","AGE_AT_DIAGNOSIS","OS_MONTHS","OS_STATUS","THREEGENE","VITAL_STATUS","LATERALITY","RADIO_THERAPY","HISTOLOGICAL_SUBTYPE","BREAST_SURGERY","RFS_MONTHS","RFS_STATUS"])
# Dropnas of Meta here because we only need data of Claudine Typ, so we don't care if NA is in some other column
meta_reduced_df = meta_reduced_df.dropna()
print(f"metadata shape after: {meta_reduced_df.shape}")
print(meta_reduced_df.shape)
print(expr_transposed_df.shape)


```

## Merge Data with reduced Metadata on Patient-ID
### Change Claudine Subtypes from String to Integer 
#### LumA        --> 6   (its most often in the data so it gets the highest number)
#### LumB        --> 5     
#### Her2        --> 4  
#### claudin-low --> 3
#### Basal       --> 2
#### Normal      --> 1
#### NC          --> 0

```{python}
#set Index Name of transposed expr_data the same as meta data
expr_transposed_df.index.name = 'PATIENT_ID'
#merge both sets together 
merged_df=expr_transposed_df.merge(meta_reduced_df, how='left', on="PATIENT_ID")
#change Subtypes from String to numbers
merged_df["CLAUDIN_SUBTYPE"] = merged_df["CLAUDIN_SUBTYPE"].replace({"LumA": "6", "LumB": "5","Her2": "4","claudin-low": "3","Basal": "2","Normal": "1","NC": "0"})# inplace = True)
print(merged_df.shape)
print("Data Merged")
```


## Multivalorate Linear Regression
```{python}

#Tensor Conversion

X = merged_df.iloc[:,0:-1].values #X = all expression data

Y = merged_df["CLAUDIN_SUBTYPE"].values #Y = Claudin subtype
Y = Y.astype(int)

X = torch.tensor(X, dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.float32)


print("Conversion to tensor done")
```


```{python}

#Training

n_epochs = 124

model = nn.Linear(20603, 1)

loss_func = nn.MSELoss()


model.train()
for epoch in range(n_epochs):
    y_pred = model(X)
    loss = loss_func(y_pred, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("Fitted parameters:")
print("Intercept (bias):", model.bias.item())

## Notice more weights
print("Coefficients (weights):", model.weight.detach().numpy())
print('Loss: ', loss)
```

```{python}
#Evaluation

model.eval()
y_pred = model(X)

# convert to numpy (only for plotting)
yp = y_pred.detach().numpy()
yt = Y.numpy()

title_str = f"Truth vs Predictions. Loss = MSE = {loss:.3f}"
plt.figure(figsize=(6,4))
plt.plot(yp, yt, 'o')
plt.xlabel("y_pred = f(x)")
plt.ylabel("y_true")
plt.title(title_str)
plt.legend()
plt.grid(True)
plt.show()
plt.close()
```


## Multinomial Regression

```{python}

#Tensor Conversion

X = merged_df.iloc[:-400,0:-1].values #X = all gene expression data

X_test = merged_df.iloc[-400:, 0:-1].values 

Y = merged_df["CLAUDIN_SUBTYPE"].iloc[:-400].values #Y = Claudin subtype
Y = Y.astype(int)

Y_test = merged_df["CLAUDIN_SUBTYPE"].iloc[-400:].values
Y_test = Y_test.astype(int)


X = torch.tensor(X, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.long)
Y_test = torch.tensor(Y_test, dtype=torch.long)


print("Conversion to tensor done")
```

```{python}

#model training

n_epochs = 500
model = nn.Linear(20603, 7)  # change: n_in = 20603 (features), n_out = 7 (classes)
loss_func = nn.CrossEntropyLoss()  # Cross-Entropy

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) 

model.train()
for epoch in range(n_epochs):
    # model predictions are interpreted as z = logits
    z = model(X)  
    loss = loss_func(z, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print("Fitted parameters:")
print("Intercept (bias):", model.bias.detach().numpy())

## Notice more weights
print("Coefficients (weights):", model.weight.detach().numpy())
print('Loss: ', loss)    
```


```{python}
#Model Evaluation
model.eval()
with torch.no_grad():
    z = model(X_test)
    y_prob = z.softmax(dim=1) # for easier interpretation
    y_pred = z.argmax(dim=1)  # class predition = largest logit
    acc = (y_pred == Y_test).float().mean().item()

print('y_true:     ', Y[:7])
print('z (logits): ', z[:7])
print('y_prob:     ', y_prob[:7])
print('y_pred:     ', y_pred[:7])

print(f"\n\nTraining accuracy: {acc:.3f}")

```

```{python}
#Confusion Matrix

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

cm = confusion_matrix(Y_test, y_pred.numpy())
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=Y)
disp.plot(cmap='Blues')
```

## Deep Learning

```{python}
#Tensor Conversion

X = merged_df.iloc[:-400,0:-1].values #X = all gene expression data

X_test = merged_df.iloc[-400:, 0:-1].values 

Y = merged_df["CLAUDIN_SUBTYPE"].iloc[:-400].values #Y = Claudin subtype
Y = Y.astype(int)

Y_test = merged_df["CLAUDIN_SUBTYPE"].iloc[-400:].values
Y_test = Y_test.astype(int)


X = torch.tensor(X, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
Y = torch.tensor(Y, dtype=torch.long)
Y_test = torch.tensor(Y_test, dtype=torch.long)


print("Conversion to tensor done")
```

```{python}
#Model Definition

class Classifier(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=256 ):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(1024,512),
            nn.BatchNorm1d(512),
            nn.ReLU(),

        )

        self.latent = nn.Linear(512, hidden_dim)
        self.latent_bn = nn.BatchNorm1d(hidden_dim)
        self.classifier = nn.Linear(hidden_dim, output_dim)

#    def forward(self, x):
#        return self.net(x)

    def forward(self, x):
        # return NN(x) _and_ hidden_ state z_bn
        x = self.encoder(x)
        z = self.latent(x)
        z_bn = self.latent_bn(z)  # latent representation (approx. normal)
        out = self.classifier(z_bn)
        return out, z_bn  # return both for access later


#model = Classifier(input_dim, hidden_dim, output_dim)
```

```{python}
#model training

n_epochs = 30
losslist = []
model = Classifier(input_dim = 20603, output_dim=7)  # change: n_in = 20603 (features), n_out = 7 (classes)
loss_func = nn.CrossEntropyLoss()  # Cross-Entropy

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) 

model.train()
for epoch in range(n_epochs):
    # model predictions are interpreted as z = logits
    z_out, z_bn = model(X)  
    loss = loss_func(z_out, Y)
    losslist.append(loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   
```

```{python}
#Model Evaluation

model.eval()
with torch.no_grad():
    z_out, z_bn = model(X_test)
    y_prob = z_out.softmax(dim=1) # for easier interpretation
    y_pred = z_out.argmax(dim=1)  # class predition = largest logit
    acc = (y_pred == Y_test).float().mean().item()

print('y_true:     ', Y[:7])
#print('z (logits): ', z_out[:7])
print('y_prob:     ', y_prob[:7])
print('y_pred:     ', y_pred[:7])

print(f"\n\nTraining accuracy: {acc:.3f}")



#loss plot

xs = [x for x in range(len(losslist))]
plt.plot(xs, losslist)
plt.show()

#Confusion Matrix

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

labels = sorted(torch.unique(Y_test).tolist())

cm = confusion_matrix(Y_test, y_pred.numpy())
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap='Blues')


```

# Divide into Full dim and PCA representation